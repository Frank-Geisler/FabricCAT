{"cells":[{"cell_type":"markdown","source":["![logo](https://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE1Mu3b?ver=5c31)\n","\n","# **Fabric**\n","### Simulating streaming data for Realtime Analytics âš¡ using fabric Data Engineering notebook \n","### AKA: \"The Wood Chipper\" \n","This notebook will read any CSV you give it via the \"SampleCsv\" Parameter and will send it to an EventStream custom app endpoint (event hub). The notebook will send a certain number of lines for each batches according to the \"MyBatchSize\" Parameter. The number of batch size is computed automatically according to the total number of lines and batch size and the while loop will stop once the file has been streamed completely.  "],"metadata":{},"id":"a93be367-5dd5-41a0-96d2-6db4178a2dc0"},{"cell_type":"markdown","source":["### **0. Set the parameters**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4dad526e-407d-4541-b7d2-d6af28447239"},{"cell_type":"code","source":["# The connection string is what you get from the \"custom app\" endpoint in EventStream\n","MyConnectionString = 'Endpoint=sb://eventstream-d89d32a1-5ddc-49c1-9c51-5e34f0c0681a.servicebus.windows.net/;SharedAccessKeyName=key_fff65a54-2bb7-50d0-4269-46420f78cd6e;SharedAccessKey=wh35lCT+ObYj19cpzfjF44G+mCobXM1Hi+AEhL4mgQs=;EntityPath=es_d89d32a1-5ddc-49c1-9c51-5e34f0c0681a'\n","\n","# This is the endpoint for where the CSV file is sitting.\n","SampleCsv = 'abfss://adxcseworkspace@msit-onelake.dfs.fabric.microsoft.com/QueenOfTheSky_Lh.Lakehouse/Files/QueenOfTheSky_ex.csv'\n","\n","# Set batch size (i.e. number of rows from the CSV that being sent at once. use a higher number when wanting a more rapid movement on the report)\n","MyBatchSize = 12\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0d894de5-a5a5-4dc8-b48d-5267651ab5a4"},{"cell_type":"markdown","source":["### **1. Install dependencies and Event Hub library**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bcd9c142-f852-4c63-8070-3254b142edb1"},{"cell_type":"code","source":["pip install azure-eventhub>=5.11.0"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5e09adb7-282c-474d-b31b-712a9d3273ee"},{"cell_type":"code","source":["import time\n","import os\n","import datetime\n","import json\n","import math\n","from azure.eventhub import EventHubProducerClient, EventData"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c1fe989f-60a1-4c78-83dc-f18235f0771e"},{"cell_type":"markdown","source":["### **2. Create a Python script to send events to your event stream**\n","\n","ref: https://learn.microsoft.com/azure/event-hubs/event-hubs-capture-python#create-a-python-script-to-send-events-to-your-event-hub"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7e2da62d-0b89-4cd7-8652-d17c1bc422e0"},{"cell_type":"code","source":["df = spark.read.csv(path=SampleCsv,header=True)\n","\n","producer = EventHubProducerClient.from_connection_string(conn_str=MyConnectionString)\n","\n","# Determine the row count of the file\n","z = df.count() \n","\n","i            = MyBatchSize \n","x            = 0     # We open the batch at the first row by array index so we stat at 0\n","y            = x+i   # We seal the batch at Start + Increment(i)\n","BatchCounter = 0     # Initializing a batch counter\n","RowCounter   = 0     # Initializine a Row counter\n","TargetBatchCount = z/i if z%i ==0 else math.ceil(z/i) # Adding an additional batch if (RowCount / BatchSize) has a residual to catch them.\n","print ('====================================')\n","print ('Target batch count should be: '+ str(TargetBatchCount))\n","print ('====================================')\n","print ('Beginning stream...')\n","print ('====================================')\n","\n","#while y in range(0, z):\n","#while y <= z :\n","while BatchCounter < TargetBatchCount:\n","\n","    BatchCounter = BatchCounter + 1   \n","    b = producer.create_batch()     # == Open the batch\n","    j = df.toJSON().collect()[x:y]  # == Collect Rows from x to y and convert them to JSON\n","    b.add(EventData(j))             # == Add the JSON to the payload\n","    producer.send_batch(b)          # == Send it!\n","    time.sleep(1)                   \n","    producer.close()\n","    # == Printing some stats to track the stream                \n","    print ('This was batch #:' + str(BatchCounter))\n","    print ('We loaded rows from: ' + str(x) + ' to row: ' + str(y)) \n","    # == Setting the control variable for the next pass\n","    RowCounter   = RowCounter + i\n","    RowRemaining = max(0,(z-RowCounter))\n","    x = y\n","    y = x+i if RowRemaining > i else x+RowRemaining\n","    print ('Rows remaining in the stream: ' + str(RowRemaining))\n","    print ('====================================')\n","\n","print ('====================================')  \n","print ('End of stream reached')\n","print ('====================================')    \n","print ('Number of batches was: '  + str(BatchCounter))\n","print ('Last batch was from row: '+ str(x) + ' to row: '+ str(y)) "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"editable":true,"run_control":{"frozen":false}},"id":"61509439-23b7-4247-905e-384ca8d9ecdf"}],"metadata":{"widgets":{},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[]}}},"nbformat":4,"nbformat_minor":5}